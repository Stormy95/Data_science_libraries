{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI - Keras Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La spécificité de keras est quelle utilise TensorFlow et Theano comme \"engine\". Cette librairie permet de se concentrer sur l'aspect design et training et les specificités liées au tensor sont laissé à tensorflow et theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sommaire\n",
    "\n",
    "[I. Designing a neural network](#Partie1)\n",
    "\n",
    "[II. Training a binary and multiclass classifier](#Partie2)\n",
    "- [II.1. Binary](#Partie2.1) \n",
    "- [II.2. Multiclass](#Partie2.2) \n",
    "\n",
    "[III. Training a regressor](#Partie3)\n",
    "\n",
    "[IV. Visualizing training and making predictions](#Partie4)\n",
    "- [IV.1. Visualizing training](#Partie4.1) \n",
    "- [IV.2. Making predictions](#Partie4.2) \n",
    "\n",
    "[V. Reduce overfitting](#Partie5)\n",
    "- [V.1. With weight regularization](#Partie5.1) \n",
    "- [V.2. With early stopping](#Partie5.2)\n",
    "- [V.3. With dropout](#Partie5.3) \n",
    "\n",
    "[VI. Saving model training progress](#Partie6)\n",
    "\n",
    "[VII. k-fold cross-validation and tunning a neural network](#Partie7)\n",
    "- [VII.1. K-fold cross validation](#Partie7.1) \n",
    "- [VII.2. Tuning neural networks](#Partie7.2)\n",
    "\n",
    "[VIII. Visualizing neural networks](#Partie8)\n",
    "\n",
    "[IX. Classifying images](#Partie9)\n",
    "- [IX.1. Image classification](#Partie9.1) \n",
    "- [IX.2. Improving performance with image augmentation](#Partie9.2)\n",
    "\n",
    "[X. Classifying text](#Partie10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Designing a neural network<a class=\"anchor\" id=\"Partie1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord les données sont preprocessées selon leur type (si numérique alors on utilise standardscaler)\n",
    "\n",
    "Les reseaux de neurones consistent en des couches d'unité (neurone). Pour construire ici un reseau feed forward on doit faire plusieurs choix pour l'architecture et le processus d'entrainement. Il faut tout d'abord se rappeler que chaque unit (ie neurone) recoit :\n",
    "- un certain nombe d'inputs\n",
    "- toutes les inputs sont pondérés par un paramètre\n",
    "- on fait la combinaison linéaire de toutes les inputs pondérés et d'un biais (1 en général)\n",
    "- on y applique ensuite une fonction d'activation\n",
    "- on envoie l'output a la couche suivante\n",
    "\n",
    "1/ De manière générale: plus il y a de neurones sur une couche plus le reseau sera capable d'apprendre des pattern complexes mais plus le reseau est susceptible de surappendre. On doit d'abord chosir la fonction d'activation (relu est un bon choix si la somme pondérée est inférieur à 0 alors la fonction renvoie 0 sinon elle renvoie la somme)\n",
    "\n",
    "2/ On doit ensuite définir le nombre de couche cachée à ajouter à l'architecture. Plus il y a de couche plus le reseau peut appendre des relations complexes mais avec un coût sur la complexité.\n",
    " \n",
    "3/On doit ensuite définir la couche d'activation pour la dernière couche. Cela depend du but du reseau:\n",
    "- pour une classification binaire on utilisera sigmoide\n",
    "- pour une classifcation multiclass on utilisera softmax\n",
    "- pour une regression on utilisera pas d'activation (ie linéaire)\n",
    "\n",
    "4/ Dans un 4eme temps on définit la loss function (qui mesure à quel point les valeurs prédites sont poches des vrais valeurs)\n",
    "- binary classification: binary cross entropy\n",
    "- multiclass classification: categorical cross entropy\n",
    "- regression: mse\n",
    "\n",
    "5/ On définit ensuite \"l'optimizer\" qui est note stratégie pour trouver les meilleurs paramètres pou poduire une loss minimale. les choix communs sont: stochastic gradient descent, with momentum, rms propagation, adaptative moment...\n",
    "\n",
    "6/ On selectionne les metriques de performance de notre algorithme.\n",
    "\n",
    "Keras permet de construire un reseau avec sequential qui va juxtaposer chaque couche. Dans notre reseau on a crée un reseau à 2 couches (on ne compte pas la couche d'entrée car elle n'a pas de poid). Les couches sont \"dense\" (qui correspond à des couches fully connected ie tous les neurones de la couche précédente sont connectés au neurones de la couche suivante). Units=16 signifie qu'il y a 16 neurones sur la couche. Dans Keras la première couche cachée doti comporter l'argument \"input_shape\" qui est la dimension des features (ie leur nombre). (10,) signifie que chaque observation a 10 features. ici on a designé un reseau pour une classification binaire donc il y a une seule sortie avec activation sigmoide qui sera entre 0 et 1 qui correspond à la probabilité d'obtenir la classe 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(10,))) #ajout d'une fully connected layer avec Relu\n",
    "network.add(layers.Dense(units=16,activation=\"relu\")) #ajout d'une fully connected layer avec Relu\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\")) #ajout d'une fully connected layer avec sigmoide\n",
    "\n",
    "network.compile(loss=\"binary_crossentropy\", # loss de cross entropy pour pb de classification\n",
    "                optimizer=\"rmsprop\", # propagation avec root mean square \n",
    "                metrics=[\"accuracy\"]) #accuracy comme metrique de performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Training a binary and multiclass classifier<a class=\"anchor\" id=\"Partie2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Binary<a class=\"anchor\" id=\"Partie2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet entrainement d'un modele de classification binaire on utilise les données: de movie review (text) qui sont catégorisées en bonne (1) ou mauvaise (0). On convertit les reviews en 5000 features binaire qui indique ou non la présence des 1000 mots les plus fréquents (en one-hot encoder: si mot existe 1 sinon 0). Ces one hot vecteurs correspondent au set de train et test avec leur label correspondant.\n",
    "\n",
    "On utilise la fonction fit de Keras pour entrainer le modèle:\n",
    "- 2 premiers paramètres: features et target vecteurs du set de train\n",
    "- paramètre epoch: nombre d'itération de train sur tout le set de train (ie tous les batchs)\n",
    "- verbose 0,1,2 : donne des informations plus ou moins détaillé sur les epochs\n",
    "- batch_size: taille des batchs pendant les epoch\n",
    "- test data\n",
    "\n",
    "La méthode fit de Keras renvoie un object qui contient les valeurs de loss et les metriques de performance demandées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4178 - accuracy: 0.8140 - val_loss: 0.3423 - val_accuracy: 0.8533\n",
      "Epoch 2/3\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3249 - accuracy: 0.8644 - val_loss: 0.3342 - val_accuracy: 0.8583\n",
      "Epoch 3/3\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3152 - accuracy: 0.8677 - val_loss: 0.3360 - val_accuracy: 0.8574\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "number_of_features=1000 #nombre de features dans le set\n",
    "\n",
    "#on utilise le set movie review pour créer le set de train et test\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features) \n",
    "\n",
    "#On convertit les data movie review en one-hot feature matrice encodée\n",
    "# dans les observation on ne garde que les 1000 mots les plus féquents des data set\n",
    "# en encodant les obersavtion en one hot vecteur de longeur 1000\n",
    "tokenizer=Tokenizer(num_words=number_of_features) \n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(number_of_features,))) \n",
    "network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\")) \n",
    "\n",
    "network.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# train neural network\n",
    "\n",
    "history=network.fit(features_train, #set de training\n",
    "                    target_train, #label de training\n",
    "                    epochs=3, #nombre d'epoch\n",
    "                    verbose=1, #print la description a chaque epoch\n",
    "                    batch_size=100, #nombre d'observation par batch\n",
    "                    validation_data=(features_test,target_test)) #test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Multiclass<a class=\"anchor\" id=\"Partie2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Training a regressor<a class=\"anchor\" id=\"Partie3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Visualizing training and making predictions<a class=\"anchor\" id=\"Partie4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1. Visualizing training<a class=\"anchor\" id=\"Partie4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2. Making predictions<a class=\"anchor\" id=\"Partie4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Reduce overfitting<a class=\"anchor\" id=\"Partie5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.1. With weight regularization<a class=\"anchor\" id=\"Partie5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2. With early stopping<a class=\"anchor\" id=\"Partie5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.3. With dropout<a class=\"anchor\" id=\"Partie5.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Saving model training progress<a class=\"anchor\" id=\"Partie6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. k-fold cross-validation and tunning a neural network<a class=\"anchor\" id=\"Partie7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.1. K-fold cross validation<a class=\"anchor\" id=\"Partie7.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.2. Tuning neural networks<a class=\"anchor\" id=\"Partie7.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Visualizing neural networks<a class=\"anchor\" id=\"Partie8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. Classifying images<a class=\"anchor\" id=\"Partie9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX.1. Image classification<a class=\"anchor\" id=\"Partie9.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX.2. Improving performance with image augmentation<a class=\"anchor\" id=\"Partie9.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Classifying text<a class=\"anchor\" id=\"Partie10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "facb5e61a4394193cb0f4fe6a1e15649a9df4920ef6ba80a21095838ac15af6e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
