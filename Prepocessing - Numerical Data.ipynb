{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepocessing - Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Très souvent besoinde rescale les données en entré car la plupart des algorithmes de machine learning présuppose que toutes les features sont à la même échelle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "[I. Rescaling, Standardizing, Normalizing features](#Partie1)\n",
    "\n",
    "[II. Generating polynomial and interaction features](#Partie2)\n",
    "\n",
    "[III. Detection and Handling of outliers](#Partie3)\n",
    "\n",
    "[IV. Discretizating features](#Partie4)\n",
    "\n",
    "[V. Deleting and imputing missing values](#Partie5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Rescaling, Standardizing, Normalizing features <a class=\"anchor\" id=\"Partie1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScaler : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On approxime les features de sorte à ce qu'elles suivent une distribution normal centrée réduite.\n",
    "$$\\begin{equation}\n",
    "x_{i}^{\\prime}=\\frac{x_{i}-x_{moy}}{\\sigma}\n",
    "\\end{equation} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=np.array([[-500],[-200],[0],[400],[900]]) #create feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.27194541],\n",
       "       [-0.65648795],\n",
       "       [-0.24618298],\n",
       "       [ 0.57442696],\n",
       "       [ 1.60018938]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_scale=preprocessing.StandardScaler()\n",
    "scaled_feature=standard_scale.fit_transform(feature)\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MinMaxScaler : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilise le minimum et le maximum du vecteur d'entrée pour réechelonner dans une range précisé (0,1).\n",
    "$$\\begin{equation}\n",
    "x_{i}^{\\prime}=\\frac{x_{i}-\\min (x)}{\\max (x)-\\min (x)}\n",
    "\\end{equation} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=np.array([[-500],[-200],[0],[400],[900]]) #create feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21428571],\n",
       "       [0.35714286],\n",
       "       [0.64285714],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scale=preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "scaled_feature=minmax_scale.fit_transform(feature)\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distinction entre minmax ou standardisation se fait souvent dépendamment de l'algorithme utilisé, très souvent standardiser sera préféré pour de la PCA par exemple mais minmax peut être recommandé pour des réseaux de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizer : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on souhaite normalisé (une somme des normes égale à 1) les valeurs des observations (ie plutôt qu'agir directement sur tout le jeu de donnée comme min-max ou standard on va normalisé features par features) avec la norme que l'on souhaite utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=np.array([[0.5,0.5],[-200,456],[0,7],[56,400]]) #create feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678,  0.70710678],\n",
       "       [-0.4016615 ,  0.91578821],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.13864784,  0.99034175]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer=Normalizer(norm=\"l2\")\n",
    "scaled_feature=normalizer.transform(feature)\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Generating polynomial and interaction features <a class=\"anchor\" id=\"Partie2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vient ajouter les termes du polynômes du degré mentionné (exemple avec degré=2), on vient ajouter\n",
    "$$\\begin{equation}\n",
    "x_{1}^{2} , \\space x_{1}*x_{2} , \\space x_{2}^{2}\n",
    "\\end{equation} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également choisir de ne regarder que les termes au puissance ou que les termes d'interaction ou les 2.\n",
    "Cette démarche est utile s'il existe et si l'on souhaite ajouter des relations non linéaires entre les features et la valeur à prédire. Par s'il existe un effet non constant entre l'interaction de 2 variables et la valeur à prédire (ex: l'age et une condition médical on peut par exmeple donner comme variable l'age au carré ou cube pour montrer l'impact plus  important de l'age sur une condition médical pour les valeurs hautes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=np.array([[-2,6],[1,7]]) #create feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.,   6.,   4., -12.,  36.],\n",
       "       [  1.,   7.,   1.,   7.,  49.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_interaction=PolynomialFeatures(degree=2, include_bias=False)\n",
    "feature_interaction=polynomial_interaction.fit_transform(feature)\n",
    "feature_interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Detection and Handling of outliers <a class=\"anchor\" id=\"Partie3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Détecter les outliers: EllipticEnvelope : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une solution pour la détection d'outlier est de supposer que les données sont ditribuees suivant une loi normale et de \"dessiner\" une ellipse autour de ces données et classifier toutes les données dans l'ellipse comme des \"inlier\" (1) et celle en dehors comme \"outlier\" (-1)\n",
    "\n",
    "La limitation ici est qu'il faut préciser une valeur de \"contamination\" qui est la proportion a priori d'outlier dans le jeu de donnée (valeur non connue en général)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_detector=EllipticEnvelope(contamination=0.1)\n",
    "outlier_detector.fit(feature)\n",
    "outlier_detector.predict(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Détecter les outliers: IsolationForest : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf TP 3 Machine learning detection anomalie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gestion des outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Première stratégie: dropper les outliers directement du jeu de donnée\n",
    "- Deuxième stratégie: on définit une nouvelle features \"Outlier\" qui donne un booleen (0 ou 1) si l'observation est une outleir ou pas et on peut directement utiliser cette feature dans le set de donnée\n",
    "- Troisième stratégie: On transforme la feature avec outlier pour ne plus avoir les effets d'anomalie (via standardisation, passage au log etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour savoir quelle strat utilisée cela dépend directement du \"pourquoi ces observations sont des outliers\" :\n",
    "- 1: Est ce qu'elles sont purement issu d'erreur de mesure ? alors on peut juste les dropper ou les remplacer\n",
    "- 2: Si elles sont des valeurs extrêmes réelles du jeu de donnée alors les marquer et ajouter une feature \"outlier\" est plus approprié\n",
    "\n",
    "De la même manière cela dépend aussi du but de l'algorithme de ML, il est raisonnable de penser que pour ceratines problématiques les valeurs extrêmes du jeu de données ne sont pas driver par les même facteurs que la vaste majorité des données et dépendent de facteur absent du jeu de donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RobustScaler: \n",
    "Si présence d'outlier non négligeable dans un jeu de donnée on pêut utilier pour le preprocessing des données:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Discretizating features <a class=\"anchor\" id=\"Partie4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarizer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on a des valeurs numériques mais que l'on souhaite les discrétiser pour une problématique données (ex: pic de pollution avec un seuil de concentration de gaz au dessus duquel 1 sinon 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=np.array([[6],[20],[12],[56],[78]]) #create feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer=Binarizer(threshold=18)\n",
    "binarized_feature=binarizer.fit_transform(feature)\n",
    "binarized_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Digitize: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on a des valeurs numériques mais que l'on souhaite les discrétiser pour une problématique données, ici on définit des tranches qui prendront 0,1,2 etc comme valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [3]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digitized_feature=np.digitize(feature,bins=[20,30,60])\n",
    "digitized_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Handling missing values <a class=\"anchor\" id=\"Partie5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premier cas on peut simplement supprimer les valeurs manquantes avec df.dropna() mais cela ne devrait pas être priviligié comme solution car l'algo perd alors en information et peut introduire du biais dans l'algo. On peut définir 3 types de valeurs manquantes: \n",
    "\n",
    "- 1 :(MCAR: missing completely at random): La proba qu'une valeur manque est completement indépendante de quoi que ce soit.\n",
    "- 2 :(MAR: missing at random): La proba n'est pas completement indépendante mais dépend d'une info dans une autre feature. Dans un survey la non réponse de l'interrogé peut être une information en elle même (pas envie de répondre, pas la possiblité de répondre etc)\n",
    "- 3 :(MNAR: missing not at random): La proba n'est pas indépendante mais dépend d'information non contenue dans les features\n",
    "\n",
    "Dans les cas 1 et 2 il peut être acceptable de supprimer les valeurs manquantes mais plus délicat dans le 3eme cas car injecte automatiquement du biais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Première solution: utiliser KNN si petit volume de donnée est très efficace\n",
    "- Deuxième solution: Imputer de scikit avec la moyenne (penser à ajouter une feature avec 0 ou 1 selon si une valeur était manquante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imputer=SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "feature_mean_imputed=mean_imputer.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
