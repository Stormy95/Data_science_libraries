{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI - Keras Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La spécificité de keras est quelle utilise TensorFlow et Theano comme \"engine\". Cette librairie permet de se concentrer sur l'aspect design et training et les specificités liées au tensor sont laissé à tensorflow et theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import imdb, reuters, mnist\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sommaire\n",
    "\n",
    "[I. Designing a neural network](#Partie1)\n",
    "\n",
    "[II. Training a binary and multiclass classifier](#Partie2)\n",
    "- [II.1. Binary](#Partie2.1) \n",
    "- [II.2. Multiclass](#Partie2.2) \n",
    "\n",
    "[III. Training a regressor](#Partie3)\n",
    "\n",
    "[IV. Visualizing training and making predictions](#Partie4)\n",
    "- [IV.1. Visualizing training](#Partie4.1) \n",
    "- [IV.2. Making predictions](#Partie4.2) \n",
    "\n",
    "[V. Reduce overfitting](#Partie5)\n",
    "- [V.1. With weight regularization](#Partie5.1) \n",
    "- [V.2. With early stopping](#Partie5.2)\n",
    "- [V.3. With dropout](#Partie5.3) \n",
    "\n",
    "[VI. k-fold cross-validation and tunning a neural network](#Partie6)\n",
    "- [VI.1. K-fold cross validation](#Partie6.1) \n",
    "- [VI.2. Tuning neural networks](#Partie6.2)\n",
    "\n",
    "[VII. Visualizing neural networks](#Partie7)\n",
    "\n",
    "[VIII. Classifying images](#Partie8)\n",
    "- [VIII.1. Image classification](#Partie8.1) \n",
    "- [VIII.2. Improving performance with image augmentation](#Partie8.2)\n",
    "\n",
    "[IX. Classifying text](#Partie9)\n",
    "\n",
    "[X. Saving & loading a model](#Partie10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Designing a neural network<a class=\"anchor\" id=\"Partie1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord les données sont preprocessées selon leur type (si numérique alors on utilise standardscaler)\n",
    "\n",
    "Les reseaux de neurones consistent en des couches d'unité (neurone). Pour construire ici un reseau feed forward on doit faire plusieurs choix pour l'architecture et le processus d'entrainement. Il faut tout d'abord se rappeler que chaque unit (ie neurone) recoit :\n",
    "- un certain nombe d'inputs\n",
    "- toutes les inputs sont pondérés par un paramètre\n",
    "- on fait la combinaison linéaire de toutes les inputs pondérés et d'un biais (1 en général)\n",
    "- on y applique ensuite une fonction d'activation\n",
    "- on envoie l'output a la couche suivante\n",
    "\n",
    "1/ De manière générale: plus il y a de neurones sur une couche plus le reseau sera capable d'apprendre des pattern complexes mais plus le reseau est susceptible de surappendre. On doit d'abord chosir la fonction d'activation (relu est un bon choix si la somme pondérée est inférieur à 0 alors la fonction renvoie 0 sinon elle renvoie la somme)\n",
    "\n",
    "2/ On doit ensuite définir le nombre de couche cachée à ajouter à l'architecture. Plus il y a de couche plus le reseau peut appendre des relations complexes mais avec un coût sur la complexité.\n",
    " \n",
    "3/On doit ensuite définir la couche d'activation pour la dernière couche. Cela depend du but du reseau:\n",
    "- pour une classification binaire on utilisera sigmoide\n",
    "- pour une classifcation multiclass on utilisera softmax\n",
    "- pour une regression on utilisera pas d'activation (ie linéaire)\n",
    "\n",
    "4/ Dans un 4eme temps on définit la loss function (qui mesure à quel point les valeurs prédites sont poches des vrais valeurs)\n",
    "- binary classification: binary cross entropy\n",
    "- multiclass classification: categorical cross entropy\n",
    "- regression: mse\n",
    "\n",
    "5/ On définit ensuite \"l'optimizer\" qui est note stratégie pour trouver les meilleurs paramètres pou poduire une loss minimale. les choix communs sont: stochastic gradient descent, with momentum, rms propagation, adaptative moment...\n",
    "\n",
    "6/ On selectionne les metriques de performance de notre algorithme.\n",
    "\n",
    "Keras permet de construire un reseau avec sequential qui va juxtaposer chaque couche. Dans notre reseau on a crée un reseau à 2 couches (on ne compte pas la couche d'entrée car elle n'a pas de poid). Les couches sont \"dense\" (qui correspond à des couches fully connected ie tous les neurones de la couche précédente sont connectés au neurones de la couche suivante). Units=16 signifie qu'il y a 16 neurones sur la couche. Dans Keras la première couche cachée doti comporter l'argument \"input_shape\" qui est la dimension des features (ie leur nombre). (10,) signifie que chaque observation a 10 features. ici on a designé un reseau pour une classification binaire donc il y a une seule sortie avec activation sigmoide qui sera entre 0 et 1 qui correspond à la probabilité d'obtenir la classe 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(10,))) #ajout d'une fully connected layer avec Relu\n",
    "network.add(layers.Dense(units=16,activation=\"relu\")) #ajout d'une fully connected layer avec Relu\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\")) #ajout d'une fully connected layer avec sigmoide\n",
    "\n",
    "network.compile(loss=\"binary_crossentropy\", # loss de cross entropy pour pb de classification\n",
    "                optimizer=\"rmsprop\", # propagation avec root mean square \n",
    "                metrics=[\"accuracy\"]) #accuracy comme metrique de performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Training a binary and multiclass classifier<a class=\"anchor\" id=\"Partie2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Binary<a class=\"anchor\" id=\"Partie2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet entrainement d'un modele de classification binaire on utilise les données: de movie review (text) qui sont catégorisées en bonne (1) ou mauvaise (0). On convertit les reviews en 5000 features binaire qui indique ou non la présence des 1000 mots les plus fréquents (en one-hot encoder: si mot existe 1 sinon 0). Ces one hot vecteurs correspondent au set de train et test avec leur label correspondant.\n",
    "\n",
    "On utilise la fonction fit de Keras pour entrainer le modèle:\n",
    "- 2 premiers paramètres: features et target vecteurs du set de train\n",
    "- paramètre epoch: nombre d'itération de train sur tout le set de train (ie tous les batchs)\n",
    "- verbose 0,1,2 : donne des informations plus ou moins détaillé sur les epochs\n",
    "- batch_size: taille des batchs pendant les epoch\n",
    "- test data\n",
    "\n",
    "La méthode fit de Keras renvoie un object qui contient les valeurs de loss et les metriques de performance demandées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "250/250 [==============================] - 2s 5ms/step - loss: 0.4247 - accuracy: 0.8104 - val_loss: 0.3370 - val_accuracy: 0.8578\n",
      "Epoch 2/3\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3261 - accuracy: 0.8638 - val_loss: 0.3490 - val_accuracy: 0.8503\n",
      "Epoch 3/3\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3156 - accuracy: 0.8687 - val_loss: 0.3269 - val_accuracy: 0.8612\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "number_of_features=1000 #nombre de features dans le set\n",
    "\n",
    "#on utilise le set movie review pour créer le set de train et test\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features) \n",
    "\n",
    "#On convertit les data movie review en one-hot feature matrice encodée\n",
    "# dans les observation on ne garde que les 1000 mots les plus féquents des data set\n",
    "# en encodant les obersavtion en one hot vecteur de longeur 1000\n",
    "tokenizer=Tokenizer(num_words=number_of_features) \n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(number_of_features,))) \n",
    "network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\")) \n",
    "\n",
    "network.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# train neural network\n",
    "\n",
    "history=network.fit(features_train, #set de training\n",
    "                    target_train, #label de training\n",
    "                    epochs=3, #nombre d'epoch\n",
    "                    verbose=1, #print la description a chaque epoch\n",
    "                    batch_size=100, #nombre d'observation par batch\n",
    "                    validation_data=(features_test,target_test)) #test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Multiclass<a class=\"anchor\" id=\"Partie2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La différence entre le classifier ci-dessus et dessous est la catégorisation en 46 classe plutôt que une classe 0 et une classe 1 il doit donc y avoir 46 noeuds sur la dernière couche par rapport à un noeud sur la première. Le vecteur de target doit donc également être encodé en one-hot, c'est à dire chaque ligne de ces matrices contient 46 élements avec des 0 partout sauf au numéro de la classe correspondante.\n",
    "Un élément important à prendre en compte est l'utilisation de la fonction d'activation softmax sur la dernière couche qui va permettre d'avoir une somme des issues égale à 1 (correspondant donc au probabilité d'occurence de chaque classe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "90/90 [==============================] - 2s 16ms/step - loss: 1.5288 - accuracy: 0.6627 - val_loss: 1.1213 - val_accuracy: 0.7422\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.7980 - accuracy: 0.8211 - val_loss: 0.9409 - val_accuracy: 0.7872\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.5101 - accuracy: 0.8927 - val_loss: 0.8748 - val_accuracy: 0.7974\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "number_of_features=5000 #nombre de features dans le set\n",
    "\n",
    "#on utilise le set reuters pour créer le set de train et test\n",
    "data=reuters.load_data(num_words=number_of_features) \n",
    "(data_train,target_train),(data_test,target_test)=data\n",
    "\n",
    "# On convertit les data movie review en one-hot feature matrice encodée\n",
    "# dans les observation on ne garde que les 5000 mots les plus féquents des data set\n",
    "# en encodant les obersavtion en one hot vecteur de longeur 1000\n",
    "tokenizer=Tokenizer(num_words=number_of_features) \n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "# on one-hot encode le vecteur de target\n",
    "target_train=to_categorical(target_train)\n",
    "target_test=to_categorical(target_test)\n",
    "\n",
    "\n",
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dense(units=100,activation=\"relu\",input_shape=(number_of_features,))) \n",
    "network.add(layers.Dense(units=100,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=46,activation=\"softmax\")) #units = nombre de classe dans le set\n",
    "\n",
    "network.compile(loss=\"categorical_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# train neural network\n",
    "\n",
    "history=network.fit(features_train, #set de training\n",
    "                    target_train, #label de training\n",
    "                    epochs=3, #nombre d'epoch\n",
    "                    verbose=1, #print la description a chaque epoch\n",
    "                    batch_size=100, #nombre d'observation par batch\n",
    "                    validation_data=(features_test,target_test)) #test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Training a regressor<a class=\"anchor\" id=\"Partie3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les cas précédents nous appliquions des fonctions activation sur la dernière couche. Dans le cas d'une regression on utilise une activation \"linéaire\" c'est à dire on utilise pas de fonction d'activation on renvoie directement le résultat de la combinaison linéaire de la dernière couche de neurones. On utilise ici une loss approriée comme la mse ou la mae selon la distribution des valeurs et notamment des anomalies pour leur donner plus ou moins de poid.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "67/67 [==============================] - 1s 5ms/step - loss: 17283.7715 - mse: 17283.7715 - val_loss: 17622.2363 - val_mse: 17622.2363\n",
      "Epoch 2/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 16387.2734 - mse: 16387.2734 - val_loss: 16313.6318 - val_mse: 16313.6318\n",
      "Epoch 3/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 14727.8926 - mse: 14727.8926 - val_loss: 14159.8477 - val_mse: 14159.8477\n",
      "Epoch 4/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 12265.2920 - mse: 12265.2920 - val_loss: 11216.7344 - val_mse: 11216.7344\n",
      "Epoch 5/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 9179.9346 - mse: 9179.9346 - val_loss: 7859.4199 - val_mse: 7859.4199\n",
      "Epoch 6/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 5942.0479 - mse: 5942.0479 - val_loss: 4629.0483 - val_mse: 4629.0483\n",
      "Epoch 7/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 3283.6599 - mse: 3283.6599 - val_loss: 2389.8352 - val_mse: 2389.8352\n",
      "Epoch 8/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 1615.6670 - mse: 1615.6670 - val_loss: 1115.0990 - val_mse: 1115.0990\n",
      "Epoch 9/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 752.1387 - mse: 752.1387 - val_loss: 531.9846 - val_mse: 531.9846\n",
      "Epoch 10/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 444.3413 - mse: 444.3413 - val_loss: 358.0852 - val_mse: 358.0852\n",
      "Epoch 11/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 305.9139 - mse: 305.9139 - val_loss: 243.8978 - val_mse: 243.8978\n",
      "Epoch 12/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 198.9214 - mse: 198.9214 - val_loss: 154.6958 - val_mse: 154.6958\n",
      "Epoch 13/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 134.9572 - mse: 134.9572 - val_loss: 112.1084 - val_mse: 112.1084\n",
      "Epoch 14/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 99.7500 - mse: 99.7500 - val_loss: 80.0157 - val_mse: 80.0157\n",
      "Epoch 15/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 67.2452 - mse: 67.2452 - val_loss: 51.9975 - val_mse: 51.9975\n",
      "Epoch 16/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 39.5460 - mse: 39.5460 - val_loss: 26.1007 - val_mse: 26.1007\n",
      "Epoch 17/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 18.4223 - mse: 18.4223 - val_loss: 9.9754 - val_mse: 9.9754\n",
      "Epoch 18/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 5.8725 - mse: 5.8725 - val_loss: 2.2964 - val_mse: 2.2964\n",
      "Epoch 19/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 1.5442 - mse: 1.5442 - val_loss: 0.8549 - val_mse: 0.8549\n",
      "Epoch 20/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.8986 - mse: 0.8986 - val_loss: 0.9616 - val_mse: 0.9616\n",
      "Epoch 21/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.6841 - mse: 0.6841 - val_loss: 0.8826 - val_mse: 0.8826\n",
      "Epoch 22/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.5693 - mse: 0.5693 - val_loss: 0.3552 - val_mse: 0.3552\n",
      "Epoch 23/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.5013 - mse: 0.5013 - val_loss: 0.3052 - val_mse: 0.3052\n",
      "Epoch 24/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.4651 - mse: 0.4651 - val_loss: 0.3923 - val_mse: 0.3923\n",
      "Epoch 25/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.4214 - mse: 0.4214 - val_loss: 0.2780 - val_mse: 0.2780\n",
      "Epoch 26/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.4128 - mse: 0.4128 - val_loss: 0.3301 - val_mse: 0.3301\n",
      "Epoch 27/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.3798 - mse: 0.3798 - val_loss: 0.2018 - val_mse: 0.2018\n",
      "Epoch 28/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.3702 - mse: 0.3702 - val_loss: 0.5044 - val_mse: 0.5044\n",
      "Epoch 29/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.3605 - mse: 0.3605 - val_loss: 0.4092 - val_mse: 0.4092\n",
      "Epoch 30/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.3418 - mse: 0.3418 - val_loss: 0.2440 - val_mse: 0.2440\n",
      "Epoch 31/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.3402 - mse: 0.3402 - val_loss: 0.2098 - val_mse: 0.2098\n",
      "Epoch 32/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.3192 - mse: 0.3192 - val_loss: 0.4718 - val_mse: 0.4718\n",
      "Epoch 33/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.3127 - mse: 0.3127 - val_loss: 0.1253 - val_mse: 0.1253\n",
      "Epoch 34/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.3354 - mse: 0.3354 - val_loss: 0.1181 - val_mse: 0.1181\n",
      "Epoch 35/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.2877 - mse: 0.2877 - val_loss: 0.2857 - val_mse: 0.2857\n",
      "Epoch 36/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.2936 - mse: 0.2936 - val_loss: 0.1554 - val_mse: 0.1554\n",
      "Epoch 37/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.3033 - mse: 0.3033 - val_loss: 0.3879 - val_mse: 0.3879\n",
      "Epoch 38/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.3013 - mse: 0.3013 - val_loss: 0.2617 - val_mse: 0.2617\n",
      "Epoch 39/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.2597 - mse: 0.2597 - val_loss: 0.7626 - val_mse: 0.7626\n",
      "Epoch 40/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.2825 - mse: 0.2825 - val_loss: 0.3335 - val_mse: 0.3335\n",
      "Epoch 41/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.2863 - mse: 0.2863 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "Epoch 42/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.2670 - mse: 0.2670 - val_loss: 0.3337 - val_mse: 0.3337\n",
      "Epoch 43/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.1116 - val_mse: 0.1116\n",
      "Epoch 44/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2085 - val_mse: 0.2085\n",
      "Epoch 45/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.2886 - mse: 0.2886 - val_loss: 0.0978 - val_mse: 0.0978\n",
      "Epoch 46/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.2676 - mse: 0.2676 - val_loss: 0.1335 - val_mse: 0.1335\n",
      "Epoch 47/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.4359 - val_mse: 0.4359\n",
      "Epoch 48/50\n",
      "67/67 [==============================] - 0s 4ms/step - loss: 0.2541 - mse: 0.2541 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 49/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.2623 - mse: 0.2623 - val_loss: 0.1814 - val_mse: 0.1814\n",
      "Epoch 50/50\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.2480 - mse: 0.2480 - val_loss: 0.3369 - val_mse: 0.3369\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "\n",
    "#on génere les matrices de features et de target\n",
    "features,target=make_regression(n_samples=10000,\n",
    "                                n_features=3,\n",
    "                                n_informative=3,\n",
    "                                n_targets=1,\n",
    "                                noise=0,\n",
    "                                random_state=0)\n",
    "\n",
    "# On divise notre matrice et nos targets en train et test\n",
    "features_train, features_test, target_train,target_test=train_test_split(features, target, test_size=0.33, random_state=0)\n",
    "\n",
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dense(units=32,activation=\"relu\",input_shape=(features_train.shape[1],))) \n",
    "network.add(layers.Dense(units=32,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1))\n",
    "\n",
    "network.compile(loss=\"mse\", \n",
    "                optimizer=\"RMSprop\", \n",
    "                metrics=[\"mse\"])\n",
    "\n",
    "# train neural network\n",
    "\n",
    "history=network.fit(features_train, #set de training\n",
    "                    target_train, #label de training\n",
    "                    epochs=50, #nombre d'epoch\n",
    "                    verbose=1, #print la description a chaque epoch\n",
    "                    batch_size=100, #nombre d'observation par batch\n",
    "                    validation_data=(features_test,target_test)) #test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Visualizing training and making predictions<a class=\"anchor\" id=\"Partie4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 2s 4ms/step - loss: 0.4137 - accuracy: 0.8180 - val_loss: 0.3345 - val_accuracy: 0.8578\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3244 - accuracy: 0.8631 - val_loss: 0.3332 - val_accuracy: 0.8574\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3147 - accuracy: 0.8682 - val_loss: 0.3287 - val_accuracy: 0.8595\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3069 - accuracy: 0.8707 - val_loss: 0.3361 - val_accuracy: 0.8550\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.2991 - accuracy: 0.8743 - val_loss: 0.3294 - val_accuracy: 0.8578\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.2892 - accuracy: 0.8771 - val_loss: 0.3247 - val_accuracy: 0.8600\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.2788 - accuracy: 0.8828 - val_loss: 0.3279 - val_accuracy: 0.8580\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.2679 - accuracy: 0.8887 - val_loss: 0.3337 - val_accuracy: 0.8556\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.2552 - accuracy: 0.8944 - val_loss: 0.3354 - val_accuracy: 0.8557\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.2428 - accuracy: 0.8998 - val_loss: 0.3515 - val_accuracy: 0.8497\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.2297 - accuracy: 0.9064 - val_loss: 0.3609 - val_accuracy: 0.8516\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.2163 - accuracy: 0.9130 - val_loss: 0.3702 - val_accuracy: 0.8475\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.2042 - accuracy: 0.9200 - val_loss: 0.3830 - val_accuracy: 0.8457\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.1916 - accuracy: 0.9274 - val_loss: 0.4029 - val_accuracy: 0.8449\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.1780 - accuracy: 0.9325 - val_loss: 0.4352 - val_accuracy: 0.8385\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.1661 - accuracy: 0.9382 - val_loss: 0.4552 - val_accuracy: 0.8419\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.1542 - accuracy: 0.9439 - val_loss: 0.4723 - val_accuracy: 0.8418\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.1422 - accuracy: 0.9484 - val_loss: 0.4940 - val_accuracy: 0.8383\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.9538 - val_loss: 0.5390 - val_accuracy: 0.8377\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.1209 - accuracy: 0.9574 - val_loss: 0.5527 - val_accuracy: 0.8366\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.1093 - accuracy: 0.9621 - val_loss: 0.5851 - val_accuracy: 0.8313\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0992 - accuracy: 0.9673 - val_loss: 0.6277 - val_accuracy: 0.8298\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0885 - accuracy: 0.9714 - val_loss: 0.6810 - val_accuracy: 0.8298\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0803 - accuracy: 0.9745 - val_loss: 0.7191 - val_accuracy: 0.8295\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0719 - accuracy: 0.9794 - val_loss: 0.7527 - val_accuracy: 0.8276\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0645 - accuracy: 0.9812 - val_loss: 0.8045 - val_accuracy: 0.8276\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0573 - accuracy: 0.9842 - val_loss: 0.8487 - val_accuracy: 0.8244\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0510 - accuracy: 0.9864 - val_loss: 0.9325 - val_accuracy: 0.8219\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0458 - accuracy: 0.9883 - val_loss: 0.9607 - val_accuracy: 0.8256\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0400 - accuracy: 0.9902 - val_loss: 1.0190 - val_accuracy: 0.8227\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "number_of_features=1000 #nombre de features dans le set\n",
    "\n",
    "#on utilise le set movie review pour créer le set de train et test\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features) \n",
    "\n",
    "#On convertit les data movie review en one-hot feature matrice encodée\n",
    "# dans les observation on ne garde que les 1000 mots les plus féquents des data set\n",
    "# en encodant les obersavtion en one hot vecteur de longeur 1000\n",
    "tokenizer=Tokenizer(num_words=number_of_features) \n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(number_of_features,))) \n",
    "network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\")) \n",
    "\n",
    "network.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# train neural network\n",
    "\n",
    "history=network.fit(features_train, #set de training\n",
    "                    target_train, #label de training\n",
    "                    epochs=30, #nombre d'epoch\n",
    "                    verbose=1, #print la description a chaque epoch\n",
    "                    batch_size=100, #nombre d'observation par batch\n",
    "                    validation_data=(features_test,target_test)) #test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1. Visualizing training<a class=\"anchor\" id=\"Partie4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendant l'entrainement du resueau de neurones les 2 erreurs de test et de train vont diminuer. Mais à un certain point le réseau va sur-apprendre le set de train et se se \"rémorer\" celui-ci. C'est alors que l'on voit une divergence de croissance entre l'erreur de training et celle de test.  Il faut donc arreter l'entrainement avant cet instant là pour avoir une erreur de validation minmale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on recupère la loss de training et de validation et le nombre d'epoch\n",
    "training_loss=history.history[\"loss\"]\n",
    "test_loss=history.history[\"val_loss\"]\n",
    "epoch_count=range(1,len(training_loss)+1)\n",
    "\n",
    "# visualisation de la loss\n",
    "plt.plot(epoch_count[:15],training_loss[:15],\"r--\")\n",
    "plt.plot(epoch_count[:15],test_loss[:15],\"b-\")\n",
    "plt.legend([\"training\",\"test\"])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_acc=history.history[\"accuracy\"]\n",
    "test_acc=history.history[\"val_accuracy\"]\n",
    "\n",
    "# visualisation de l'accuracy\n",
    "plt.plot(epoch_count[:15],training_acc[:15],\"r--\")\n",
    "plt.plot(epoch_count[:15],test_acc[:15],\"b-\")\n",
    "plt.legend([\"training\",\"test\"])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"mse\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2. Making predictions<a class=\"anchor\" id=\"Partie4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le réseau entrainé on utilise la fonction predict pour prédire de nouvelle valeur avec le reseau. si le reseau prédit l'issu d'une classification binaire (notre cas) le résultat est la probabilité d'obtenir 1. Si c'est une multiclassification on a alors la probabilité de toutes les classes et on peut obtenir la classe avec la probabilité maximum en utilisant la fonction argmax. Enfin si c'est une régression on va avoir la valeur réelle prédite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999884], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_target=network.predict(features_test)\n",
    "predicted_target[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Reduce overfitting<a class=\"anchor\" id=\"Partie5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.1. With weight regularization<a class=\"anchor\" id=\"Partie5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on va réentrainer un classifieur binaire en utilisant une régularisation en \"pénalisant\" les paramètres (ie les poids les neurones) de sorte à ce qu'ils soient forcés à être des petites valeurs (ce qui conduit à un modèle plus simple et donc moins susceptible d'overfit). On force les poids à être plus petit car lors du calcul du gradient on ne va plus seulement soustraire le learning rate * dloss mais on va également soustraire 2 * lambda * w \n",
    "\n",
    "Cette méthode s'appelle le Weight Decay/Regularization. Cela consiste à rajouter à la loss la norme L2 des poids à chaque couche (ie la somme de chaque poid au carré pour chaque couche de neurones) multiplié par un coefficient (ici 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 2s 4ms/step - loss: 0.6458 - accuracy: 0.7991 - val_loss: 0.5006 - val_accuracy: 0.8435\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4638 - accuracy: 0.8535 - val_loss: 0.4596 - val_accuracy: 0.8484\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4379 - accuracy: 0.8542 - val_loss: 0.4520 - val_accuracy: 0.8426\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4214 - accuracy: 0.8586 - val_loss: 0.4395 - val_accuracy: 0.8438\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4116 - accuracy: 0.8568 - val_loss: 0.4235 - val_accuracy: 0.8484\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4055 - accuracy: 0.8604 - val_loss: 0.4373 - val_accuracy: 0.8380\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3978 - accuracy: 0.8616 - val_loss: 0.3960 - val_accuracy: 0.8593\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3917 - accuracy: 0.8623 - val_loss: 0.3908 - val_accuracy: 0.8606\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3861 - accuracy: 0.8619 - val_loss: 0.3897 - val_accuracy: 0.8594\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3802 - accuracy: 0.8640 - val_loss: 0.3818 - val_accuracy: 0.8606\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3749 - accuracy: 0.8633 - val_loss: 0.3794 - val_accuracy: 0.8598\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3716 - accuracy: 0.8665 - val_loss: 0.3813 - val_accuracy: 0.8582\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3696 - accuracy: 0.8655 - val_loss: 0.3793 - val_accuracy: 0.8577\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3673 - accuracy: 0.8650 - val_loss: 0.3768 - val_accuracy: 0.8584\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3655 - accuracy: 0.8653 - val_loss: 0.3728 - val_accuracy: 0.8598\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3624 - accuracy: 0.8664 - val_loss: 0.3809 - val_accuracy: 0.8544\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3613 - accuracy: 0.8675 - val_loss: 0.3767 - val_accuracy: 0.8572\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3587 - accuracy: 0.8687 - val_loss: 0.3714 - val_accuracy: 0.8596\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3587 - accuracy: 0.8669 - val_loss: 0.3706 - val_accuracy: 0.8595\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3567 - accuracy: 0.8678 - val_loss: 0.3694 - val_accuracy: 0.8603\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3561 - accuracy: 0.8693 - val_loss: 0.3682 - val_accuracy: 0.8620\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3534 - accuracy: 0.8694 - val_loss: 0.3741 - val_accuracy: 0.8556\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3528 - accuracy: 0.8711 - val_loss: 0.3703 - val_accuracy: 0.8595\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3513 - accuracy: 0.8714 - val_loss: 0.3719 - val_accuracy: 0.8592\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3500 - accuracy: 0.8711 - val_loss: 0.3686 - val_accuracy: 0.8608\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3492 - accuracy: 0.8713 - val_loss: 0.3755 - val_accuracy: 0.8563\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3473 - accuracy: 0.8711 - val_loss: 0.3709 - val_accuracy: 0.8598\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3461 - accuracy: 0.8730 - val_loss: 0.3708 - val_accuracy: 0.8598\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3453 - accuracy: 0.8736 - val_loss: 0.3955 - val_accuracy: 0.8463\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3440 - accuracy: 0.8729 - val_loss: 0.3874 - val_accuracy: 0.8528\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "number_of_features=1000 #nombre de features dans le set\n",
    "\n",
    "#on utilise le set movie review pour créer le set de train et test\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features) \n",
    "\n",
    "#On convertit les data movie review en one-hot feature matrice encodée\n",
    "# dans les observation on ne garde que les 1000 mots les plus féquents des data set\n",
    "# en encodant les obersavtion en one hot vecteur de longeur 1000\n",
    "tokenizer=Tokenizer(num_words=number_of_features) \n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",kernel_regularizer=regularizers.l2(0.01),input_shape=(number_of_features,))) \n",
    "network.add(layers.Dense(units=16,activation=\"relu\",kernel_regularizer=regularizers.l2(0.01)))\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\")) \n",
    "\n",
    "network.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# train neural network\n",
    "\n",
    "history=network.fit(features_train, #set de training\n",
    "                    target_train, #label de training\n",
    "                    epochs=30, #nombre d'epoch\n",
    "                    verbose=1, #print la description a chaque epoch\n",
    "                    batch_size=100, #nombre d'observation par batch\n",
    "                    validation_data=(features_test,target_test)) #test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2. With early stopping<a class=\"anchor\" id=\"Partie5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant réduire l'overfitting en arrêtant l'entraînement lorsque la loss du set de test arrête de décroître cela est appelé le: early stopping. De même on va réentrainer un classifieur binaire.\n",
    "\n",
    "En effet au bout d'un certain nombre d'epoch le réseau va \"mémoriser\" le set de train donc l'érreur sur le train va continuer à décroître mais celle de test va augmenter c'est pourquoi une des meilleurs méthodes est d'arreter l'entraînement du réseau quand l'erreur de test commence à augmenter. On utilise donc les callbacks qui vont être des fonctions qui vont s'appliquer à la fin de chaque epoch. Ici on applique early stopping et avec le paramètre patience au bout de 2 epochs où la loss de test a augmenter l'entraînement s'arrète. On doit rajouter SaveMyCheckpoint comme fonction callback pour bien enregistrer le meilleur modèle et donc pas le modèle après les 2 epochs où la loss de test va augmenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.4306 - accuracy: 0.8121 - val_loss: 0.3492 - val_accuracy: 0.8520\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8662 - val_loss: 0.3267 - val_accuracy: 0.8625\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3146 - accuracy: 0.8703 - val_loss: 0.3486 - val_accuracy: 0.8513\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3077 - accuracy: 0.8719 - val_loss: 0.3257 - val_accuracy: 0.8605\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.2993 - accuracy: 0.8754 - val_loss: 0.3252 - val_accuracy: 0.8594\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.2906 - accuracy: 0.8777 - val_loss: 0.3302 - val_accuracy: 0.8576\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.2823 - accuracy: 0.8813 - val_loss: 0.3285 - val_accuracy: 0.8601\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "number_of_features=1000 #nombre de features dans le set\n",
    "\n",
    "#on utilise le set movie review pour créer le set de train et test\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features) \n",
    "\n",
    "#On convertit les data movie review en one-hot feature matrice encodée\n",
    "# dans les observation on ne garde que les 1000 mots les plus féquents des data set\n",
    "# en encodant les obersavtion en one hot vecteur de longeur 1000\n",
    "tokenizer=Tokenizer(num_words=number_of_features) \n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(number_of_features,))) \n",
    "network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\")) \n",
    "\n",
    "network.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# On fixe les fonctions de callbacks qui vont permettre le early stopping \n",
    "# (ici on arrete au bout de 2 epoch quand la loss de validation n'a pas augmenté et on sauvegarde le meilleur modèle)\n",
    "\n",
    "callbacks=[EarlyStopping(monitor=\"val_loss\",patience=2),\n",
    "            ModelCheckpoint(filepath=\"best_model.h5\",monitor=\"val_loss\",save_best_only=True)]\n",
    "\n",
    "# train neural network\n",
    "\n",
    "history=network.fit(features_train, #set de training\n",
    "                    target_train, #label de training\n",
    "                    epochs=30, #nombre d'epoch\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1, #print la description a chaque epoch\n",
    "                    batch_size=100, #nombre d'observation par batch\n",
    "                    validation_data=(features_test,target_test)) #test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.3. With dropout<a class=\"anchor\" id=\"Partie5.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant réduire l'overfitting en utilisant le DropOut. De même on va réentrainer un classifieur binaire.\n",
    "\n",
    "Le Dropout est une méthode puissante et populaire pour régularizer (ie réduire l'overfitting) un réseau de neurones. Avec cette méthode à chaque fois qu'un batch d'observation est créé pour l'entraînement du réseau, une proportion définie de unit (ie neurone) est \"dropped\" ie le poid est mis à 0. Cela veut dire que lors de notre entraînement les batchs sont entraînés sur le même réseau de neurones mais à chaque batch l'architecture est différentes étant donné que les poids mis à 0 ne sont pas systématiquement les même à chaque poid.\n",
    "\n",
    "Le Dropout est efficace car en mettant à 0 le poids de certain neurones de manière constante et aléatoire cela force les neurones à apprendre des paramètres qui peuvent performer sur une variété plus grande d'architecture. Cela signifie qu'ils sont robustes face au bruit que pourrait introduire d'autres neurones ils ne mémorisent donc plus le set de training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.6400 - accuracy: 0.6238 - val_loss: 0.4949 - val_accuracy: 0.8158\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.5129 - accuracy: 0.7630 - val_loss: 0.3847 - val_accuracy: 0.8468\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4530 - accuracy: 0.8051 - val_loss: 0.3597 - val_accuracy: 0.8497\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4317 - accuracy: 0.8198 - val_loss: 0.3464 - val_accuracy: 0.8562\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4140 - accuracy: 0.8260 - val_loss: 0.3390 - val_accuracy: 0.8575\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4101 - accuracy: 0.8273 - val_loss: 0.3366 - val_accuracy: 0.8596\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.4008 - accuracy: 0.8351 - val_loss: 0.3449 - val_accuracy: 0.8540\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4030 - accuracy: 0.8301 - val_loss: 0.3379 - val_accuracy: 0.8601\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3914 - accuracy: 0.8396 - val_loss: 0.3361 - val_accuracy: 0.8581\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3945 - accuracy: 0.8360 - val_loss: 0.3319 - val_accuracy: 0.8608\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3931 - accuracy: 0.8397 - val_loss: 0.3321 - val_accuracy: 0.8608\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3986 - accuracy: 0.8373 - val_loss: 0.3388 - val_accuracy: 0.8560\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3910 - accuracy: 0.8392 - val_loss: 0.3315 - val_accuracy: 0.8576\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3889 - accuracy: 0.8385 - val_loss: 0.3301 - val_accuracy: 0.8599\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3846 - accuracy: 0.8431 - val_loss: 0.3326 - val_accuracy: 0.8591\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3869 - accuracy: 0.8409 - val_loss: 0.3320 - val_accuracy: 0.8580\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3876 - accuracy: 0.8412 - val_loss: 0.3334 - val_accuracy: 0.8586\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3806 - accuracy: 0.8442 - val_loss: 0.3318 - val_accuracy: 0.8586\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3817 - accuracy: 0.8413 - val_loss: 0.3328 - val_accuracy: 0.8600\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3833 - accuracy: 0.8429 - val_loss: 0.3364 - val_accuracy: 0.8558\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3838 - accuracy: 0.8431 - val_loss: 0.3370 - val_accuracy: 0.8582\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3791 - accuracy: 0.8445 - val_loss: 0.3343 - val_accuracy: 0.8588\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.3817 - accuracy: 0.8443 - val_loss: 0.3329 - val_accuracy: 0.8599\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.3757 - accuracy: 0.8464 - val_loss: 0.3334 - val_accuracy: 0.8605\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3801 - accuracy: 0.8454 - val_loss: 0.3329 - val_accuracy: 0.8594\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3792 - accuracy: 0.8470 - val_loss: 0.3340 - val_accuracy: 0.8600\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3773 - accuracy: 0.8471 - val_loss: 0.3339 - val_accuracy: 0.8592\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3774 - accuracy: 0.8477 - val_loss: 0.3357 - val_accuracy: 0.8583\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3756 - accuracy: 0.8451 - val_loss: 0.3347 - val_accuracy: 0.8596\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3786 - accuracy: 0.8444 - val_loss: 0.3360 - val_accuracy: 0.8594\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "number_of_features=1000 #nombre de features dans le set\n",
    "\n",
    "#on utilise le set movie review pour créer le set de train et test\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features) \n",
    "\n",
    "#On convertit les data movie review en one-hot feature matrice encodée\n",
    "# dans les observation on ne garde que les 1000 mots les plus féquents des data set\n",
    "# en encodant les obersavtion en one hot vecteur de longeur 1000\n",
    "tokenizer=Tokenizer(num_words=number_of_features) \n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dropout(0.2,input_shape=(number_of_features,))) # Dropout for input layer\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",)) \n",
    "\n",
    "network.add(layers.Dropout(0.5))  # Dropout for hidden layer\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",))\n",
    "\n",
    "network.add(layers.Dropout(0.5)) # Dropout for hidden layer\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\")) \n",
    "\n",
    "network.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# train neural network\n",
    "\n",
    "history=network.fit(features_train, #set de training\n",
    "                    target_train, #label de training\n",
    "                    epochs=30, #nombre d'epoch\n",
    "                    verbose=1, #print la description a chaque epoch\n",
    "                    batch_size=100, #nombre d'observation par batch\n",
    "                    validation_data=(features_test,target_test)) #test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. k-fold cross-validation and tunning a neural network<a class=\"anchor\" id=\"Partie6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.1. K-fold cross validation<a class=\"anchor\" id=\"Partie6.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise ne général assez peu la validation croisée sur les réseaux de neurones car leur entraînement sur un large set de donnée peut prendr énormément de temps donc utiliser une k-fold validation croisée va multiplier par k ce temps d'entraînement. Avec un large set de donnée on évaluera directement sur un set de test.\n",
    "\n",
    "Cependant sur un set de donnée plus restreint il est intéressant d'utiliser la validation croisée pour maximiser notre abilité à évaluer la perfroamnce d'une réseau de neurones. Pour cela on peut \"wrap\" n'importe quel réseau de neurones Keras pour utiliser les fonctionnalités sur scikit-learn (KerasClassifier ou https://github.com/adriangb/scikeras). Cela permet d'utiliser notre réseau de neurones Keras comme n'importe quel modèle scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-f97467753713>:29: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  neural_network=KerasClassifier(build_fn=create_network,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.51497006, 0.52852851, 0.61561561])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "number_of_features=100 #nombre de features dans le set\n",
    "\n",
    "# On génère une matrice de features et de target\n",
    "features, target=make_classification(n_samples=1000,\n",
    "                                     n_features=number_of_features,\n",
    "                                     n_informative=3,\n",
    "                                     n_redundant=0,\n",
    "                                     n_classes=2,\n",
    "                                     weights=[0.5,0.5],\n",
    "                                     random_state=0)\n",
    "\n",
    "# Créons une fonction qui renvoie un réseau compilé\n",
    "def create_network():\n",
    "\n",
    "    network=models.Sequential() #start neural network\n",
    "\n",
    "    network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(number_of_features,))) \n",
    "    network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "    network.add(layers.Dense(units=1,activation=\"sigmoid\")) \n",
    "\n",
    "    network.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "    \n",
    "    return network\n",
    "\n",
    "# On change le réseau keras pour qu'il soit utilisable par scikit-learn\n",
    "neural_network=KerasClassifier(build_fn=create_network,\n",
    "                               epochs=10,\n",
    "                               batch_size=100,\n",
    "                               verbose=0)\n",
    "\n",
    "# On évalue le réseau de neurones avec une cross validation a 3 folds\n",
    "cross_val_score(neural_network,features,target,cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.2. Tuning neural networks<a class=\"anchor\" id=\"Partie6.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est très important de correctement sélectionner les hyperparamètres d'un modèle. Cependant il est important de comprendre que chercher les hyperparamètres optimaux peu prendre énormément de temps (pour un training de qlq heures la recherche peu prendre plusieur jours/semaines). Ici on utilise la recherche dans une grille par validation croisée en définissant les valeurs des hyperparamètres que l'on souhaite tester (hyperparameters).\n",
    "\n",
    "Rien que notre modèle de classification binaire ici a pris qlq minutes pour faire la recherche d'hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-175913ef4263>:29: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  neural_network=KerasClassifier(build_fn=create_network,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 27 calls to <function Model.make_test_function.<locals>.test_function at 0x000001F712DFB5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 29 calls to <function Model.make_test_function.<locals>.test_function at 0x000001F709191DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 5, 'epochs': 10, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "number_of_features=100 #nombre de features dans le set\n",
    "\n",
    "# On génère une matrice de features et de target\n",
    "features, target=make_classification(n_samples=1000,\n",
    "                                     n_features=number_of_features,\n",
    "                                     n_informative=3,\n",
    "                                     n_redundant=0,\n",
    "                                     n_classes=2,\n",
    "                                     weights=[0.5,0.5],\n",
    "                                     random_state=0)\n",
    "\n",
    "# Créons une fonction qui renvoie un réseau compilé\n",
    "def create_network(optimizer=\"rmsprop\"):\n",
    "\n",
    "    network=models.Sequential() #start neural network\n",
    "\n",
    "    network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(number_of_features,))) \n",
    "    network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "    network.add(layers.Dense(units=1,activation=\"sigmoid\")) \n",
    "\n",
    "    network.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "    \n",
    "    return network\n",
    "\n",
    "# On change le réseau keras pour qu'il soit utilisable par scikit-learn\n",
    "neural_network=KerasClassifier(build_fn=create_network,\n",
    "                               epochs=10,\n",
    "                               batch_size=100,\n",
    "                               verbose=0)\n",
    "\n",
    "# On crée la liste des hyperparamètres\n",
    "epochs=[5,10]\n",
    "batches=[5,10,100]\n",
    "optimizers=[\"rmsprop\",\"adam\"]\n",
    "hyperparamters=dict(optimizer=optimizers,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batches)\n",
    "\n",
    "# On crée la grille de recherche de cross validation\n",
    "grid=GridSearchCV(estimator=neural_network,param_grid=hyperparamters)\n",
    "\n",
    "# On fit la grid search\n",
    "grid_result=grid.fit(features,target)\n",
    "\n",
    "# Visualisons les résultats\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Visualizing neural networks<a class=\"anchor\" id=\"Partie7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il peut être intéressant de visualiser notre modèle pour être sûr de la concordance entre les input et output de 2 couches successives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"311pt\" height=\"274pt\" viewBox=\"0.00 0.00 345.00 304.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.902778 0.902778) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-300 341,-300 341,4 -4,4\"/>\n<!-- 2160663954528 -->\n<g id=\"node1\" class=\"node\"><title>2160663954528</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-249.5 0,-295.5 337,-295.5 337,-249.5 0,-249.5\"/>\n<text text-anchor=\"middle\" x=\"56\" y=\"-268.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_216_input</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"112,-249.5 112,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"150.5\" y=\"-268.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">InputLayer</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"189,-249.5 189,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"217\" y=\"-280.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"189,-272.5 245,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"217\" y=\"-257.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"245,-249.5 245,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"291\" y=\"-280.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, 100)]</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"245,-272.5 337,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"291\" y=\"-257.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, 100)]</text>\n</g>\n<!-- 2160657855200 -->\n<g id=\"node2\" class=\"node\"><title>2160657855200</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"35,-166.5 35,-212.5 302,-212.5 302,-166.5 35,-166.5\"/>\n<text text-anchor=\"middle\" x=\"73.5\" y=\"-185.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_216</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"112,-166.5 112,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"137.5\" y=\"-185.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"163,-166.5 163,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"191\" y=\"-197.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"163,-189.5 219,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"191\" y=\"-174.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"219,-166.5 219,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"260.5\" y=\"-197.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 100)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"219,-189.5 302,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"260.5\" y=\"-174.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 16)</text>\n</g>\n<!-- 2160663954528&#45;&gt;2160657855200 -->\n<g id=\"edge1\" class=\"edge\"><title>2160663954528-&gt;2160657855200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M168.5,-249.366C168.5,-241.152 168.5,-231.658 168.5,-222.725\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"172,-222.607 168.5,-212.607 165,-222.607 172,-222.607\"/>\n</g>\n<!-- 2160657854912 -->\n<g id=\"node3\" class=\"node\"><title>2160657854912</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"38,-83.5 38,-129.5 299,-129.5 299,-83.5 38,-83.5\"/>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-102.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_217</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"115,-83.5 115,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"140.5\" y=\"-102.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"166,-83.5 166,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"194\" y=\"-114.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"166,-106.5 222,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"194\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"222,-83.5 222,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"260.5\" y=\"-114.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 16)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"222,-106.5 299,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"260.5\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 16)</text>\n</g>\n<!-- 2160657855200&#45;&gt;2160657854912 -->\n<g id=\"edge2\" class=\"edge\"><title>2160657855200-&gt;2160657854912</title>\n<path fill=\"none\" stroke=\"black\" d=\"M168.5,-166.366C168.5,-158.152 168.5,-148.658 168.5,-139.725\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"172,-139.607 168.5,-129.607 165,-139.607 172,-139.607\"/>\n</g>\n<!-- 2160657810960 -->\n<g id=\"node4\" class=\"node\"><title>2160657810960</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"38,-0.5 38,-46.5 299,-46.5 299,-0.5 38,-0.5\"/>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_218</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"115,-0.5 115,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"140.5\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"166,-0.5 166,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"194\" y=\"-31.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"166,-23.5 222,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"194\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"222,-0.5 222,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"260.5\" y=\"-31.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 16)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"222,-23.5 299,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"260.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 1)</text>\n</g>\n<!-- 2160657854912&#45;&gt;2160657810960 -->\n<g id=\"edge3\" class=\"edge\"><title>2160657854912-&gt;2160657810960</title>\n<path fill=\"none\" stroke=\"black\" d=\"M168.5,-83.3664C168.5,-75.1516 168.5,-65.6579 168.5,-56.7252\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"172,-56.6068 168.5,-46.6068 165,-56.6069 172,-56.6068\"/>\n</g>\n</g>\n</svg>",
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(number_of_features,))) \n",
    "network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\")) \n",
    "\n",
    "SVG(model_to_dot(network, show_shapes=True, show_layer_names=True, dpi=65).create(prog=\"dot\",format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Classifying images<a class=\"anchor\" id=\"Partie8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII.1. Image classification<a class=\"anchor\" id=\"Partie8.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNets sont un type de réseau très populaire qui ont prouvé leur efficacité dans la computer vision. Il serait possible d'utiliser des réseaux denses ou tous les pixels sont une features mais il y a 2 problèmes. Les réseaux denses ne prennent pas en compte la structure spatiale des pixels. De plus les réseaux denses apprenent de manières globale cela veut dire qu'un objet serait détecté sans prendre en compte sa position dans l'image.\n",
    "\n",
    "Le pouvoir des réseaux convolutionnels est donc leur habilité de gérer ces 2 problèmes. Une image a 3 dimensions, hauteur, largeur et profondeur (channels) qui représente les canaux de couleur (gris ou RGB). Une convolution peut donc être représentée par une fénètre avec un poid par pixel qui va glisser sur l'image en regardant un pixel individuel ainsi que ses voisins et en réalisant une combinaison linéaire de ces pixels selon les poids appris. La convolution va donner une nouvelle image 3D ou les 2 premières dimensions sont la largeur et la hauteur et la 3eme est le nombre de filtre \"filters\" utilisé.\n",
    "\n",
    "Le 2ème concept important ici est la pooling layer qui va permettre de réduire la dimension de l'image \"downsizing\" en faisant glisser également une fénêtre sur la nouvelle image obtenue (méthode la plus commune est le max pooling qui va prendre le pixel le plus important parmis ceux de la fenêtre). Le pooling permettre de réduire le nombre de paramètre à apprendre. \n",
    "\n",
    "Enfin les couches denses sont utilisées pour réaliser la classification.\n",
    "\n",
    "Le processus ici ets d'utiliser le MNIST dataset (handwritten digit), on va d'abord réorganiser la shape des images pour qu'elle convienne à Keras: NHWC. Ensuite on réechellone les pixels entre 0 et 1 pour ne pas avoir trop de différence entre les pixels car le modèle peut vite donner trop d'importance aux fortes valeurs de pixels. Enfin on one-hot encode les catégories.\n",
    "\n",
    "On utilise une couche de dropout pour éviter tout sur-apprentissage.\n",
    "On utilise une couche de flatten pour convertir les images convoluées en un format utilisable pour une couche dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60/60 [==============================] - 40s 647ms/step - loss: 0.5909 - accuracy: 0.8156 - val_loss: 0.1832 - val_accuracy: 0.9456\n",
      "Epoch 2/5\n",
      "60/60 [==============================] - 38s 627ms/step - loss: 0.1977 - accuracy: 0.9416 - val_loss: 0.0857 - val_accuracy: 0.9740\n",
      "Epoch 3/5\n",
      "60/60 [==============================] - 38s 640ms/step - loss: 0.1274 - accuracy: 0.9628 - val_loss: 0.0612 - val_accuracy: 0.9810\n",
      "Epoch 4/5\n",
      "60/60 [==============================] - 37s 621ms/step - loss: 0.0987 - accuracy: 0.9716 - val_loss: 0.0505 - val_accuracy: 0.9840\n",
      "Epoch 5/5\n",
      "60/60 [==============================] - 43s 712ms/step - loss: 0.0840 - accuracy: 0.9746 - val_loss: 0.0434 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "from keras import backend\n",
    "backend.set_image_data_format('channels_last')\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# configuration des informations\n",
    "channels=1\n",
    "height=28\n",
    "width=28\n",
    "\n",
    "# chargement et reshaping des données\n",
    "(data_train,target_train),(data_test,target_test)=mnist.load_data()\n",
    "\n",
    "data_train=data_train.reshape(data_train.shape[0],height,width,channels)\n",
    "data_test=data_test.reshape(data_test.shape[0],height,width,channels)\n",
    "\n",
    "# normalisation des pixels\n",
    "features_train=data_train/255\n",
    "features_test=data_test/255\n",
    "\n",
    "# encodage one-hot des labels\n",
    "target_train=np_utils.to_categorical(target_train)\n",
    "target_test=np_utils.to_categorical(target_test)\n",
    "number_of_classes=target_test.shape[1]\n",
    "\n",
    "# construction du réseau de neurones convolutionnels\n",
    "network=models.Sequential() \n",
    "\n",
    "# couche convolutionnelle avec 64 filtres, chaque filtre est de taille 5x5 et une fonction d'activation ReLU\n",
    "network.add(Conv2D(filters=64,\n",
    "                   kernel_size=(5,5),\n",
    "                   input_shape=(height,width,channels),\n",
    "                   activation='relu'))\n",
    "\n",
    "# couche de maxpooling avec un filtre de taille 2x2\n",
    "network.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Flatten())\n",
    "network.add(Dense(128,activation=\"relu\"))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Dense(number_of_classes,activation=\"softmax\"))\n",
    "\n",
    "network.compile(loss=\"categorical_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history=network.fit(features_train, #set de training\n",
    "                    target_train, #label de training\n",
    "                    epochs=5, #nombre d'epoch\n",
    "                    verbose=1, #print la description a chaque epoch\n",
    "                    batch_size=1000, #nombre d'observation par batch\n",
    "                    validation_data=(features_test,target_test)) #test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"350pt\" height=\"574pt\" viewBox=\"0.00 0.00 388.00 636.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.902778 0.902778) rotate(0) translate(4 632)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-632 384,-632 384,4 -4,4\"/>\n<!-- 2162036549424 -->\n<g id=\"node1\" class=\"node\"><title>2162036549424</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"16,-581.5 16,-627.5 364,-627.5 364,-581.5 16,-581.5\"/>\n<text text-anchor=\"middle\" x=\"63\" y=\"-600.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">conv2d_input</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"110,-581.5 110,-627.5 \"/>\n<text text-anchor=\"middle\" x=\"148.5\" y=\"-600.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">InputLayer</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"187,-581.5 187,-627.5 \"/>\n<text text-anchor=\"middle\" x=\"215\" y=\"-612.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"187,-604.5 243,-604.5 \"/>\n<text text-anchor=\"middle\" x=\"215\" y=\"-589.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"243,-581.5 243,-627.5 \"/>\n<text text-anchor=\"middle\" x=\"303.5\" y=\"-612.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, 28, 28, 1)]</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"243,-604.5 364,-604.5 \"/>\n<text text-anchor=\"middle\" x=\"303.5\" y=\"-589.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, 28, 28, 1)]</text>\n</g>\n<!-- 2160717317216 -->\n<g id=\"node2\" class=\"node\"><title>2160717317216</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"41,-498.5 41,-544.5 339,-544.5 339,-498.5 41,-498.5\"/>\n<text text-anchor=\"middle\" x=\"70.5\" y=\"-517.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">conv2d</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"100,-498.5 100,-544.5 \"/>\n<text text-anchor=\"middle\" x=\"132\" y=\"-517.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Conv2D</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"164,-498.5 164,-544.5 \"/>\n<text text-anchor=\"middle\" x=\"192\" y=\"-529.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"164,-521.5 220,-521.5 \"/>\n<text text-anchor=\"middle\" x=\"192\" y=\"-506.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"220,-498.5 220,-544.5 \"/>\n<text text-anchor=\"middle\" x=\"279.5\" y=\"-529.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 28, 28, 1)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"220,-521.5 339,-521.5 \"/>\n<text text-anchor=\"middle\" x=\"279.5\" y=\"-506.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 24, 24, 64)</text>\n</g>\n<!-- 2162036549424&#45;&gt;2160717317216 -->\n<g id=\"edge1\" class=\"edge\"><title>2162036549424-&gt;2160717317216</title>\n<path fill=\"none\" stroke=\"black\" d=\"M190,-581.366C190,-573.152 190,-563.658 190,-554.725\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"193.5,-554.607 190,-544.607 186.5,-554.607 193.5,-554.607\"/>\n</g>\n<!-- 2160451865328 -->\n<g id=\"node3\" class=\"node\"><title>2160451865328</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-415.5 0,-461.5 380,-461.5 380,-415.5 0,-415.5\"/>\n<text text-anchor=\"middle\" x=\"52\" y=\"-434.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">max_pooling2d</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"104,-415.5 104,-461.5 \"/>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-434.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">MaxPooling2D</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"205,-415.5 205,-461.5 \"/>\n<text text-anchor=\"middle\" x=\"233\" y=\"-446.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"205,-438.5 261,-438.5 \"/>\n<text text-anchor=\"middle\" x=\"233\" y=\"-423.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"261,-415.5 261,-461.5 \"/>\n<text text-anchor=\"middle\" x=\"320.5\" y=\"-446.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 24, 24, 64)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"261,-438.5 380,-438.5 \"/>\n<text text-anchor=\"middle\" x=\"320.5\" y=\"-423.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 12, 12, 64)</text>\n</g>\n<!-- 2160717317216&#45;&gt;2160451865328 -->\n<g id=\"edge2\" class=\"edge\"><title>2160717317216-&gt;2160451865328</title>\n<path fill=\"none\" stroke=\"black\" d=\"M190,-498.366C190,-490.152 190,-480.658 190,-471.725\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"193.5,-471.607 190,-461.607 186.5,-471.607 193.5,-471.607\"/>\n</g>\n<!-- 2160663954720 -->\n<g id=\"node4\" class=\"node\"><title>2160663954720</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"32.5,-332.5 32.5,-378.5 347.5,-378.5 347.5,-332.5 32.5,-332.5\"/>\n<text text-anchor=\"middle\" x=\"70.5\" y=\"-351.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dropout_3</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"108.5,-332.5 108.5,-378.5 \"/>\n<text text-anchor=\"middle\" x=\"140.5\" y=\"-351.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dropout</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"172.5,-332.5 172.5,-378.5 \"/>\n<text text-anchor=\"middle\" x=\"200.5\" y=\"-363.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"172.5,-355.5 228.5,-355.5 \"/>\n<text text-anchor=\"middle\" x=\"200.5\" y=\"-340.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"228.5,-332.5 228.5,-378.5 \"/>\n<text text-anchor=\"middle\" x=\"288\" y=\"-363.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 12, 12, 64)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"228.5,-355.5 347.5,-355.5 \"/>\n<text text-anchor=\"middle\" x=\"288\" y=\"-340.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 12, 12, 64)</text>\n</g>\n<!-- 2160451865328&#45;&gt;2160663954720 -->\n<g id=\"edge3\" class=\"edge\"><title>2160451865328-&gt;2160663954720</title>\n<path fill=\"none\" stroke=\"black\" d=\"M190,-415.366C190,-407.152 190,-397.658 190,-388.725\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"193.5,-388.607 190,-378.607 186.5,-388.607 193.5,-388.607\"/>\n</g>\n<!-- 2160451863504 -->\n<g id=\"node5\" class=\"node\"><title>2160451863504</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"50.5,-249.5 50.5,-295.5 329.5,-295.5 329.5,-249.5 50.5,-249.5\"/>\n<text text-anchor=\"middle\" x=\"75.5\" y=\"-268.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">flatten</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"100.5,-249.5 100.5,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"127.5\" y=\"-268.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Flatten</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"154.5,-249.5 154.5,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"182.5\" y=\"-280.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"154.5,-272.5 210.5,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"182.5\" y=\"-257.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"210.5,-249.5 210.5,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"270\" y=\"-280.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 12, 12, 64)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"210.5,-272.5 329.5,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"270\" y=\"-257.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 9216)</text>\n</g>\n<!-- 2160663954720&#45;&gt;2160451863504 -->\n<g id=\"edge4\" class=\"edge\"><title>2160663954720-&gt;2160451863504</title>\n<path fill=\"none\" stroke=\"black\" d=\"M190,-332.366C190,-324.152 190,-314.658 190,-305.725\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"193.5,-305.607 190,-295.607 186.5,-305.607 193.5,-305.607\"/>\n</g>\n<!-- 2160661632672 -->\n<g id=\"node6\" class=\"node\"><title>2160661632672</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"53,-166.5 53,-212.5 327,-212.5 327,-166.5 53,-166.5\"/>\n<text text-anchor=\"middle\" x=\"91.5\" y=\"-185.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_219</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"130,-166.5 130,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"155.5\" y=\"-185.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"181,-166.5 181,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"209\" y=\"-197.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"181,-189.5 237,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"209\" y=\"-174.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"237,-166.5 237,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-197.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 9216)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"237,-189.5 327,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-174.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 128)</text>\n</g>\n<!-- 2160451863504&#45;&gt;2160661632672 -->\n<g id=\"edge5\" class=\"edge\"><title>2160451863504-&gt;2160661632672</title>\n<path fill=\"none\" stroke=\"black\" d=\"M190,-249.366C190,-241.152 190,-231.658 190,-222.725\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"193.5,-222.607 190,-212.607 186.5,-222.607 193.5,-222.607\"/>\n</g>\n<!-- 2160661632960 -->\n<g id=\"node7\" class=\"node\"><title>2160661632960</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"50.5,-83.5 50.5,-129.5 329.5,-129.5 329.5,-83.5 50.5,-83.5\"/>\n<text text-anchor=\"middle\" x=\"88.5\" y=\"-102.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dropout_4</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"126.5,-83.5 126.5,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"158.5\" y=\"-102.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dropout</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"190.5,-83.5 190.5,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-114.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"190.5,-106.5 246.5,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"246.5,-83.5 246.5,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"288\" y=\"-114.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 128)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"246.5,-106.5 329.5,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"288\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 128)</text>\n</g>\n<!-- 2160661632672&#45;&gt;2160661632960 -->\n<g id=\"edge6\" class=\"edge\"><title>2160661632672-&gt;2160661632960</title>\n<path fill=\"none\" stroke=\"black\" d=\"M190,-166.366C190,-158.152 190,-148.658 190,-139.725\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"193.5,-139.607 190,-129.607 186.5,-139.607 193.5,-139.607\"/>\n</g>\n<!-- 2160418525968 -->\n<g id=\"node8\" class=\"node\"><title>2160418525968</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"56.5,-0.5 56.5,-46.5 323.5,-46.5 323.5,-0.5 56.5,-0.5\"/>\n<text text-anchor=\"middle\" x=\"95\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_220</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"133.5,-0.5 133.5,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"159\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"184.5,-0.5 184.5,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"212.5\" y=\"-31.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"184.5,-23.5 240.5,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"212.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"240.5,-0.5 240.5,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-31.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 128)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"240.5,-23.5 323.5,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 10)</text>\n</g>\n<!-- 2160661632960&#45;&gt;2160418525968 -->\n<g id=\"edge7\" class=\"edge\"><title>2160661632960-&gt;2160418525968</title>\n<path fill=\"none\" stroke=\"black\" d=\"M190,-83.3664C190,-75.1516 190,-65.6579 190,-56.7252\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"193.5,-56.6068 190,-46.6068 186.5,-56.6069 193.5,-56.6068\"/>\n</g>\n</g>\n</svg>",
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "SVG(model_to_dot(network, show_shapes=True, show_layer_names=True, dpi=65).create(prog=\"dot\",format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII.2. Improving performance with image augmentation<a class=\"anchor\" id=\"Partie8.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour améliorer la performance d'un réseau convolutionnel est de pre-process les images. ImageDataGenerator permet de faire les opérations de bases de standardisation. L'autre méthode est d'ajouter du bruit à l'image en effet cela va rendre le réseau de neurones plus robuste au bruit introduit dans des vrais images et empêche donc le sur-apprentissage sur le set de train. L'ajout de bruit se fait en transformant de manière aléatoire les images en entrée: rotation, zoom, inversion etc... ici flow_from_directory renvoie un generator il faut donc utiliser fit_generator pour entraîner notre réseau sur ces images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "# On crée  l'image augmentation\n",
    "augmentation = ImageDataGenerator(featurewise_center=True, # ZCA whitening\n",
    "                                  zoom_range=0.3, # randomly zoom on image\n",
    "                                  width_shift_range=0.2, # randomly shift image\n",
    "                                  horizontal_flip=True, # randomly flip image\n",
    "                                  rotation_range=90) # randomly rotate\n",
    "\n",
    "augment_images=augmentation.flow_from_directory(\"raw/images\",\n",
    "                                                batch_size=32,\n",
    "                                                class_mode=\"binary\",\n",
    "                                                save_to_dir=\"processed/images\")\n",
    "\n",
    "\n",
    "# history=network.fit_generator(augment_images, #set de training\n",
    "#                     steps_per_epoch=2000, #label de training\n",
    "#                     epochs=5, #nombre d'epoch\n",
    "#                     verbose=1, #print la description a chaque epoch\n",
    "#                     batch_size=1000, #nombre d'observation par batch\n",
    "#                     validation_data=augment_images_test,\n",
    "#                     validation_steps=8000) #test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. Classifying text<a class=\"anchor\" id=\"Partie9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concernant de la classification de text les réseaux récurrents sont les plus intéressants. En effet l'élément clé de ces réseaux est qu'il y a une \"mémoire\" pour la gestion de donnée séquentielle comme des phrases. Le type populaire des réseaux récurrents est LSTM (Long Short Term Memory). Ici on revient sur les movie review pour prédire si ces reviews sont négatives ou positives.\n",
    "\n",
    "On va ici utiliser Layers.Embedding pour obtenir un vecteur propre à chaque mot et ne pas utiliser un one-hot vecteur par mot qui donnerait de moins bons résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25/25 [==============================] - 418s 16s/step - loss: 0.6988 - accuracy: 0.5999 - val_loss: 0.6551 - val_accuracy: 0.6341\n",
      "Epoch 2/3\n",
      "25/25 [==============================] - 387s 16s/step - loss: 0.6167 - accuracy: 0.7020 - val_loss: 0.4754 - val_accuracy: 0.7899\n",
      "Epoch 3/3\n",
      "25/25 [==============================] - 527s 21s/step - loss: 0.4971 - accuracy: 0.7695 - val_loss: 0.5763 - val_accuracy: 0.6796\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) #on confidure la random seed\n",
    "number_of_features=1000 #nombre de features dans le set (ie nombre de mot)\n",
    "\n",
    "# on utilise le set movie review pour créer le set de train et test afin d'avoir le même nombre de feature par review\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features) \n",
    "\n",
    "# on tronque les observations à 400 features \n",
    "features_train=sequence.pad_sequences(data_train,maxlen=400)\n",
    "features_test=sequence.pad_sequences(data_test,maxlen=400)\n",
    "\n",
    "network=models.Sequential() #start neural network\n",
    "\n",
    "network.add(layers.Embedding(input_dim=number_of_features, output_dim=128)) # embedding layer\n",
    "\n",
    "network.add(layers.LSTM(units=128)) # LSTM with 128 units\n",
    "\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\")) \n",
    "\n",
    "network.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"rmsprop\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# train neural network\n",
    "\n",
    "history=network.fit(features_train, #set de training\n",
    "                    target_train, #label de training\n",
    "                    epochs=1, #nombre d'epoch\n",
    "                    verbose=1, #print la description a chaque epoch\n",
    "                    batch_size=1000, #nombre d'observation par batch\n",
    "                    validation_data=(features_test,target_test)) #test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"321pt\" height=\"274pt\" viewBox=\"0.00 0.00 356.00 304.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.902778 0.902778) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-300 352,-300 352,4 -4,4\"/>\n<!-- 2160918986608 -->\n<g id=\"node1\" class=\"node\"><title>2160918986608</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-249.5 0,-295.5 348,-295.5 348,-249.5 0,-249.5\"/>\n<text text-anchor=\"middle\" x=\"56.5\" y=\"-268.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">embedding_input</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"113,-249.5 113,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"151.5\" y=\"-268.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">InputLayer</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"190,-249.5 190,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"218\" y=\"-280.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"190,-272.5 246,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"218\" y=\"-257.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"246,-249.5 246,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"297\" y=\"-280.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, None)]</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"246,-272.5 348,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"297\" y=\"-257.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, None)]</text>\n</g>\n<!-- 2160681012336 -->\n<g id=\"node2\" class=\"node\"><title>2160681012336</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"6.5,-166.5 6.5,-212.5 341.5,-212.5 341.5,-166.5 6.5,-166.5\"/>\n<text text-anchor=\"middle\" x=\"45.5\" y=\"-185.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">embedding</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"84.5,-166.5 84.5,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"124.5\" y=\"-185.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Embedding</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"164.5,-166.5 164.5,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"192.5\" y=\"-197.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"164.5,-189.5 220.5,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"192.5\" y=\"-174.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"220.5,-166.5 220.5,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"281\" y=\"-197.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, None)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"220.5,-189.5 341.5,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"281\" y=\"-174.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, None, 128)</text>\n</g>\n<!-- 2160918986608&#45;&gt;2160681012336 -->\n<g id=\"edge1\" class=\"edge\"><title>2160918986608-&gt;2160681012336</title>\n<path fill=\"none\" stroke=\"black\" d=\"M174,-249.366C174,-241.152 174,-231.658 174,-222.725\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"177.5,-222.607 174,-212.607 170.5,-222.607 177.5,-222.607\"/>\n</g>\n<!-- 2160918985360 -->\n<g id=\"node3\" class=\"node\"><title>2160918985360</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"38.5,-83.5 38.5,-129.5 309.5,-129.5 309.5,-83.5 38.5,-83.5\"/>\n<text text-anchor=\"middle\" x=\"58.5\" y=\"-102.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lstm</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"78.5,-83.5 78.5,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"105.5\" y=\"-102.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">LSTM</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"132.5,-83.5 132.5,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"160.5\" y=\"-114.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"132.5,-106.5 188.5,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"160.5\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"188.5,-83.5 188.5,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"249\" y=\"-114.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, None, 128)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"188.5,-106.5 309.5,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"249\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 128)</text>\n</g>\n<!-- 2160681012336&#45;&gt;2160918985360 -->\n<g id=\"edge2\" class=\"edge\"><title>2160681012336-&gt;2160918985360</title>\n<path fill=\"none\" stroke=\"black\" d=\"M174,-166.366C174,-158.152 174,-148.658 174,-139.725\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"177.5,-139.607 174,-129.607 170.5,-139.607 177.5,-139.607\"/>\n</g>\n<!-- 2160918986080 -->\n<g id=\"node4\" class=\"node\"><title>2160918986080</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"40.5,-0.5 40.5,-46.5 307.5,-46.5 307.5,-0.5 40.5,-0.5\"/>\n<text text-anchor=\"middle\" x=\"79\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_221</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"117.5,-0.5 117.5,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"143\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"168.5,-0.5 168.5,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-31.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"168.5,-23.5 224.5,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"224.5,-0.5 224.5,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"266\" y=\"-31.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 128)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"224.5,-23.5 307.5,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"266\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 1)</text>\n</g>\n<!-- 2160918985360&#45;&gt;2160918986080 -->\n<g id=\"edge3\" class=\"edge\"><title>2160918985360-&gt;2160918986080</title>\n<path fill=\"none\" stroke=\"black\" d=\"M174,-83.3664C174,-75.1516 174,-65.6579 174,-56.7252\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"177.5,-56.6068 174,-46.6068 170.5,-56.6069 177.5,-56.6068\"/>\n</g>\n</g>\n</svg>",
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(network, show_shapes=True, show_layer_names=True, dpi=65).create(prog=\"dot\",format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Saving & loading a model<a class=\"anchor\" id=\"Partie10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save(\"model_embedding_review.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_network=load_model(\"model_embedding_review.h5\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "facb5e61a4394193cb0f4fe6a1e15649a9df4920ef6ba80a21095838ac15af6e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
